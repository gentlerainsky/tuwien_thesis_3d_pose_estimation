{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pose_2d\n",
                "# pose_3d"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import pickle\n",
                "import json\n",
                "import numpy as np\n",
                "\n",
                "keypoint_2d_path = ''\n",
                "keypoint_3d_path = ''\n",
                "\n",
                "synthetic_cabin_ir_1m_root_path = Path('/root/data/processed/synthetic_cabin_1m/') / 'all_views'\n",
                "drive_and_act_root_path = Path('/root/data/processed/drive_and_act/') / 'inner_mirror'\n",
                "\n",
                "synthetic_cabin_ir_1m_keypoint_2d_path = synthetic_cabin_ir_1m_root_path / 'annotations' / 'person_keypoints_train.json'\n",
                "drive_and_act_keypoint_2d_path = drive_and_act_root_path / 'keypoint_detection_results' / 'keypoint_detection_train.json'\n",
                "keypoint_3d_path = synthetic_cabin_ir_1m_root_path / 'annotations'\n",
                "bbox_path = synthetic_cabin_ir_1m_root_path / 'person_detection_results'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "pose_2d = []\n",
                "pose_3d = []\n",
                "train_actors = ['vp1', 'vp2', 'vp3', 'vp4', 'vp5', 'vp6', 'vp7', 'vp8']\n",
                "views = set(['Dashboard', 'Front', 'OMS_01'])\n",
                "synthetic_data_mapper = {}\n",
                "with open(synthetic_cabin_ir_1m_root_path / 'annotations' / 'person_keypoints_train.pkl', 'rb') as f:\n",
                "    synthetic_data = pickle.load(f)\n",
                "    for item in synthetic_data['annotations']:\n",
                "        synthetic_data_mapper[item['image_id']] = dict(\n",
                "            pose_2d=np.array(item['keypoints']).reshape(-1, 3)[:,:2],\n",
                "            pose_3d=np.array(item['keypoints3D']).reshape(-1, 3)\n",
                "        )\n",
                "\n",
                "for item in synthetic_data['images']:\n",
                "    if item['view'] in views:\n",
                "        pose_2d.append(synthetic_data_mapper[item['id']]['pose_2d'])\n",
                "        pose_3d.append(synthetic_data_mapper[item['id']]['pose_3d'])\n",
                "\n",
                "drive_and_act_kps_mapper = {}\n",
                "with open(drive_and_act_keypoint_2d_path) as f:\n",
                "    drive_and_act_kps = json.loads(f.read())\n",
                "    for item in drive_and_act_kps:\n",
                "        drive_and_act_kps_mapper[item['image_id']] = np.array(item['keypoints']).reshape(-1, 3)[:,:2]\n",
                "\n",
                "with open(drive_and_act_root_path / 'annotations' / 'person_keypoints_train.json') as f:\n",
                "    drive_and_act_anns = json.loads(f.read())\n",
                "\n",
                "for item in drive_and_act_anns['images']:\n",
                "    if item['actor'] in train_actors:\n",
                "        pose_2d.append(drive_and_act_kps_mapper[item['id']])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "synthetic_cabin_ir_1m_v2_dataset_root_path = Path('/root/data/processed/synthetic_cabin_1m/')\n",
                "keypoint_3d_path = synthetic_cabin_ir_1m_v2_dataset_root_path / 'all_views' / 'annotations'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Global seed set to 1234\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pytorch_lightning as pl\n",
                "from pytorch_lightning.callbacks import ModelCheckpoint\n",
                "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
                "from torch.nn import functional as F\n",
                "from torch.utils.data import DataLoader, random_split\n",
                "from modules.lifter_2d_3d.dataset.gan_keypoint_dataset import GANKeypointDataset\n",
                "from modules.lifter_2d_3d.dataset.synthetic_cabin_ir_1m_dataset import SyntheticCabinIR1MKeypointDataset\n",
                "from modules.lifter_2d_3d.dataset.drive_and_act_keypoint_dataset import DriveAndActKeypointDataset\n",
                "\n",
                "from modules.lifter_2d_3d.model.linear_model.lit_linear_model import BaselineModel\n",
                "from modules.lifter_2d_3d.model.repnet.lit_repnet import LitRepNet\n",
                "from modules.utils.visualization import (\n",
                "    generate_connection_line, get_sample_from_loader, visualize_pose\n",
                ")\n",
                "from IPython.display import display\n",
                "\n",
                "pl.seed_everything(1234)\n",
                "\n",
                "train_dataset = GANKeypointDataset(\n",
                "    pose_2d,\n",
                "    pose_3d,\n",
                "    subset_percentage=5\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "train_dataset 8750 val_dataset 87500 test_dataset 11017\n"
                    ]
                }
            ],
            "source": [
                "val_dataset = SyntheticCabinIR1MKeypointDataset(\n",
                "    prediction_file=(synthetic_cabin_ir_1m_root_path / 'annotations' / 'person_keypoints_val.json').as_posix(),\n",
                "    annotation_file=(synthetic_cabin_ir_1m_root_path / 'annotations' / 'person_keypoints_val.json').as_posix(),\n",
                "    bbox_file=(synthetic_cabin_ir_1m_root_path / 'person_detection_results' / f'ground_truth_human_detection_val.json').as_posix(),\n",
                "    image_width=1280,\n",
                "    image_height=1024,\n",
                "    exclude_ankle=True,\n",
                "    exclude_knee=True,\n",
                "    bbox_format='xyxy',\n",
                "    is_center_to_neck=True,\n",
                "    is_normalize_to_bbox=False,\n",
                "    is_normalize_to_pose=True,\n",
                "    is_normalize_rotation=True,\n",
                "    is_gt_2d_pose=True,\n",
                "    included_view='Dashboard_Front_OMS_01',\n",
                "    subset_percentage=100\n",
                ")\n",
                "test_dataset = DriveAndActKeypointDataset(\n",
                "    prediction_file=(drive_and_act_root_path / 'keypoint_detection_results' / 'keypoint_detection_train.json').as_posix(),\n",
                "    annotation_file=(drive_and_act_root_path / 'annotations' / 'person_keypoints_train.json').as_posix(),\n",
                "    bbox_file=(drive_and_act_root_path / 'person_detection_results' / 'human_detection_train.json').as_posix(),\n",
                "    image_width=1280,\n",
                "    image_height=1024,\n",
                "    actors=['vp11', 'vp12', 'vp13', 'vp14'],\n",
                "    exclude_ankle=True,\n",
                "    exclude_knee=True,\n",
                "    bbox_format='xyxy',\n",
                "    is_center_to_neck=True,\n",
                "    is_normalize_to_bbox=False,\n",
                "    is_normalize_to_pose=True,\n",
                "    is_normalize_rotation=True\n",
                ")\n",
                "all_activities = test_dataset.activities\n",
                "print(\n",
                "    'train_dataset', len(train_dataset),\n",
                "    'val_dataset', len(val_dataset),\n",
                "    'test_dataset', len(test_dataset)\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DataModule(pl.LightningDataModule):\n",
                "    def __init__(self, train_dataset, val_dataset, test_dataset):\n",
                "        super().__init__()\n",
                "        self.train_dataset = train_dataset\n",
                "        self.val_dataset = val_dataset\n",
                "        self.test_dataset = test_dataset\n",
                "\n",
                "    def train_dataloader(self):\n",
                "        self.train_dataset.shuffle()\n",
                "        return DataLoader(self.train_dataset, batch_size=64, drop_last=True, shuffle=True, num_workers=24)\n",
                "\n",
                "    def val_dataloader(self):\n",
                "        return DataLoader(self.val_dataset, batch_size=64, drop_last=True, num_workers=24)\n",
                "\n",
                "    def test_dataloader(self):\n",
                "        return DataLoader(test_dataset, batch_size=64, num_workers=24)\n",
                "dm = DataModule(train_dataset, val_dataset, test_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "GPU available: True (cuda), used: True\n",
                        "TPU available: False, using: 0 TPU cores\n",
                        "IPU available: False, using: 0 IPUs\n",
                        "HPU available: False, using: 0 HPUs\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "device cuda\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
                        "\n",
                        "  | Name          | Type               | Params\n",
                        "-----------------------------------------------------\n",
                        "0 | lifter_2D_3D  | BaselineModel      | 4.3 M \n",
                        "1 | camera_net    | CameraNet          | 4.0 M \n",
                        "2 | generator     | RepNet             | 8.3 M \n",
                        "3 | discriminator | DiscriminatorModel | 89.2 K\n",
                        "-----------------------------------------------------\n",
                        "8.4 M     Trainable params\n",
                        "0         Non-trainable params\n",
                        "8.4 M     Total params\n",
                        "33.650    Total estimated model params size (MB)\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "71be9e79c27c4eb49554e69ab8ced850",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Sanity Checking: 0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "check #0\n",
                        "val MPJPE from: 128 samples : 2380.923271179199\n",
                        "val P-MPJPE from: 128 samples : 2226.1156506496127\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# train_loader = DataLoader(train_dataset, batch_size=64, drop_last=True, shuffle=True, num_workers=24)\n",
                "# val_loader = DataLoader(val_dataset, batch_size=64, drop_last=True, num_workers=24)\n",
                "# test_loader = DataLoader(test_dataset, batch_size=64, num_workers=24)\n",
                "\n",
                "model_checkpoint = ModelCheckpoint(monitor='mpjpe',mode='min', save_top_k=1)\n",
                "early_stopping = EarlyStopping(monitor='mpjpe', mode=\"min\", patience=2)\n",
                "\n",
                "# ------------\n",
                "# model\n",
                "# ------------\n",
                "lifter_2D_3D = BaselineModel(exclude_ankle=True, exclude_knee=True)\n",
                "lit_model = LitRepNet(\n",
                "    lifter_2D_3D=lifter_2D_3D,\n",
                "    all_activities=all_activities,\n",
                ")\n",
                "# ------------\n",
                "# training\n",
                "# ------------\n",
                "saved_model_path = './saved_lifter_2d_3d_model/rq3/repnet'\n",
                "if not os.path.exists(saved_model_path):\n",
                "    os.makedirs(saved_model_path)\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print('device', device)\n",
                "# device = 'cpu'\n",
                "trainer = pl.Trainer(\n",
                "    # max_steps=10,\n",
                "    max_epochs=100,\n",
                "    callbacks=[model_checkpoint, early_stopping],\n",
                "    accelerator=device,\n",
                "    check_val_every_n_epoch=1,\n",
                "    default_root_dir=saved_model_path,\n",
                "    # gradient_clip_val=1.0\n",
                "    reload_dataloaders_every_n_epochs=1,\n",
                "    log_every_n_steps=1\n",
                ")\n",
                "# trainer.fit(lit_model, train_loader, val_loader)\n",
                "trainer.fit(lit_model, dm)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
