{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping problematic image 3628\n",
      "skipping problematic image 5874\n",
      "skipping problematic image 14835\n",
      "skipping problematic image 15783\n",
      "skipping problematic image 17258\n",
      "skipping problematic image 17259\n",
      "skipping problematic image 21271\n",
      "skipping problematic image 21272\n",
      "skipping problematic image 21273\n",
      "skipping problematic image 21274\n",
      "skipping problematic image 21275\n",
      "skipping problematic image 21276\n",
      "skipping problematic image 32959\n",
      "skipping problematic image 33527\n",
      "skipping problematic image 28113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/modules/lifter_2d_3d/model/graformer/lit_graformer.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset 22881 val_dataset 6240 test_dataset 11017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: saved_lifter_2d_3d_model/graphmlp/drive_and_act/A_Pillar_Codriver/predicted_2d/all_actors/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | GraFormer | 926 K \n",
      "------------------------------------\n",
      "926 K     Trainable params\n",
      "0         Non-trainable params\n",
      "926 K     Total params\n",
      "3.708     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cad797f8fe4ae895e94612bc5ff2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #0\n",
      "val MPJPE from: 128 samples : 1114.497184753418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59173cfe7d964e64b422371e6b4a45c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e640624c864449a199134e5415644a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #1\n",
      "training loss from 1785 batches: 129.96534166716728\n",
      "val MPJPE from: 6208 samples : 76.69416069984436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e55f076fe24916ac75690ca10ea31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #2\n",
      "training loss from 1785 batches: 108.94380931009431\n",
      "val MPJPE from: 6208 samples : 85.85063368082047\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3c3312763b46a4b9e1fc761a137366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #3\n",
      "training loss from 1785 batches: 106.52381805729132\n",
      "val MPJPE from: 6208 samples : 76.0408565402031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3487047f63c64eb8b57016074db87332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #4\n",
      "training loss from 1785 batches: 104.1338769828572\n",
      "val MPJPE from: 6208 samples : 75.0146135687828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16437b64db304016b0ca622e38cafb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #5\n",
      "training loss from 1785 batches: 101.52614308672459\n",
      "val MPJPE from: 6208 samples : 83.07494968175888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12496bd303534745b562e722b305f8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #6\n",
      "training loss from 1785 batches: 99.07019022299129\n",
      "val MPJPE from: 6208 samples : 75.17120242118835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a92fa5c8ae40e988a75acac4ed26a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #7\n",
      "training loss from 1785 batches: 96.3573038911953\n",
      "val MPJPE from: 6208 samples : 72.94146716594696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04afa8e3604341e0a24d1044e406cda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #8\n",
      "training loss from 1785 batches: 93.89798633482943\n",
      "val MPJPE from: 6208 samples : 77.20399647951126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a3606945c44ec89b235727a8538cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #9\n",
      "training loss from 1785 batches: 91.70257975073422\n",
      "val MPJPE from: 6208 samples : 75.33077150583267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cb877ba0a74ffcb251b38b33170a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #10\n",
      "training loss from 1785 batches: 89.6321643321287\n",
      "val MPJPE from: 6208 samples : 80.35537600517273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95699a37c854620be16d8c3e1da326c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #11\n",
      "training loss from 1785 batches: 87.74142477776157\n",
      "val MPJPE from: 6208 samples : 77.18906551599503\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b238dde90f4c74b4026b003a253ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check #12\n",
      "training loss from 1785 batches: 86.19076237917281\n",
      "val MPJPE from: 6208 samples : 79.65315133333206\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import DataLoader\n",
    "from modules.lifter_2d_3d.dataset.drive_and_act_keypoint_dataset import DriveAndActKeypointDataset\n",
    "from modules.lifter_2d_3d.model.graformer.lit_graformer import LitGraformer\n",
    "from modules.utils.visualization import (\n",
    "    plot_samples\n",
    ")\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "\n",
    "pl.seed_everything(1234)\n",
    "\n",
    "# ------------\n",
    "# dataset path\n",
    "# ------------\n",
    "dataset_root_path = Path('/root/data/processed/drive_and_act/')\n",
    "keypoint_2d_path = dataset_root_path / 'keypoint_detection_results'\n",
    "keypoint_3d_path = dataset_root_path / 'annotations'\n",
    "bbox_file = dataset_root_path / 'person_detection_results'\n",
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "image_width = 1280\n",
    "image_height = 1024\n",
    "batch_size = 64\n",
    "max_epoch = 200\n",
    "val_check_period = 5\n",
    "early_stopping_patience = 5\n",
    "# ------------\n",
    "# saved model path\n",
    "# ------------\n",
    "saved_model_path = './saved_lifter_2d_3d_model/graformer/drive_and_act/A_Pillar_Codriver/predicted_2d/all_actors/'\n",
    "\n",
    "\n",
    "train_dataset = DriveAndActKeypointDataset(\n",
    "    prediction_file=(keypoint_2d_path / 'keypoint_detection_train.json').as_posix(),\n",
    "    annotation_file=(keypoint_3d_path / 'person_keypoints_train.json').as_posix(),\n",
    "    bbox_file=(bbox_file / 'human_detection_train.json').as_posix(),\n",
    "    image_width=image_width,\n",
    "    image_height=image_height,\n",
    "    # actors=['vp1', 'vp4', 'vp5', 'vp6', 'vp7', 'vp8', 'vp9', 'vp10', 'vp15'],\n",
    "    actors=['vp1', 'vp2', 'vp3', 'vp4', 'vp5', 'vp6', 'vp7', 'vp8'],\n",
    "    exclude_ankle=True,\n",
    "    exclude_knee=True,\n",
    "    # is_normalize_to_bbox=True,\n",
    "    # bbox_format='xyxy'\n",
    "\n",
    "    is_center_to_neck=True,\n",
    "    is_normalize_to_bbox=False,\n",
    "    is_normalize_to_pose=True,\n",
    "    is_normalize_rotation=True\n",
    ")\n",
    "val_dataset = DriveAndActKeypointDataset(\n",
    "    prediction_file=(keypoint_2d_path / 'keypoint_detection_train.json').as_posix(),\n",
    "    annotation_file=(keypoint_3d_path / 'person_keypoints_train.json').as_posix(),\n",
    "    bbox_file=(bbox_file / 'human_detection_train.json').as_posix(),\n",
    "    image_width=image_width,\n",
    "    image_height=image_height,\n",
    "    # actors=['vp2', 'vp3'],\n",
    "    actors=['vp9', 'vp10', 'vp15'],\n",
    "    exclude_ankle=True,\n",
    "    exclude_knee=True,\n",
    "    # is_normalize_to_bbox=True,\n",
    "    # bbox_format='xyxy'\n",
    "\n",
    "    is_center_to_neck=True,\n",
    "    is_normalize_to_bbox=False,\n",
    "    is_normalize_to_pose=True,\n",
    "    is_normalize_rotation=True\n",
    ")\n",
    "test_dataset = DriveAndActKeypointDataset(\n",
    "    prediction_file=(keypoint_2d_path / 'keypoint_detection_train.json').as_posix(),\n",
    "    annotation_file=(keypoint_3d_path / 'person_keypoints_train.json').as_posix(),\n",
    "    bbox_file=(bbox_file / 'human_detection_train.json').as_posix(),\n",
    "    image_width=image_width,\n",
    "    image_height=image_height,\n",
    "    actors=['vp11', 'vp12', 'vp13', 'vp14'],\n",
    "    # actors=['vp13', 'vp14', 'vp15'],\n",
    "    exclude_ankle=True,\n",
    "    exclude_knee=True,\n",
    "    # is_normalize_to_bbox=True,\n",
    "    # bbox_format='xyxy' \n",
    "    is_center_to_neck=True,\n",
    "    is_normalize_to_bbox=False,\n",
    "    is_normalize_to_pose=True,\n",
    "    is_normalize_rotation=True\n",
    ")\n",
    "all_activities = train_dataset.activities.union(val_dataset.activities).union(test_dataset.activities)\n",
    "lit_model = LitGraformer(exclude_ankle=True, exclude_knee=True, all_activities=all_activities,\n",
    "                        #  is_silence=False,\n",
    "                        #  learning_rate=1e-3\n",
    "                         )\n",
    "print(\n",
    "    'train_dataset', len(train_dataset),\n",
    "    'val_dataset', len(val_dataset),\n",
    "    'test_dataset', len(test_dataset)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=True, num_workers=24)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=True, num_workers=24)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=24)\n",
    "\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(monitor='val_loss',mode='min', save_top_k=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=early_stopping_patience)\n",
    "\n",
    "if not os.path.exists(saved_model_path):\n",
    "    os.makedirs(saved_model_path)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trainer = pl.Trainer(\n",
    "    # max_steps=10,\n",
    "    max_epochs=max_epoch,\n",
    "    callbacks=[model_checkpoint, early_stopping],\n",
    "    accelerator=device,\n",
    "    check_val_every_n_epoch=val_check_period,\n",
    "    default_root_dir=saved_model_path,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "trainer.fit(lit_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/modules/lifter_2d_3d/model/graformer/lit_graformer.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LitGraformer(\n",
       "  (model): GraFormer(\n",
       "    (gconv_input): ChebConv()\n",
       "    (gconv_layers): ModuleList(\n",
       "      (0-3): 4 x _ResChebGC(\n",
       "        (gconv1): _GraphConv(\n",
       "          (gconv): ChebConv()\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (gconv2): _GraphConv(\n",
       "          (gconv): ChebConv()\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (atten_layers): ModuleList(\n",
       "      (0-3): 4 x GraAttenLayer(\n",
       "        (self_attn): MultiHeadedAttention(\n",
       "          (linears): ModuleList(\n",
       "            (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): GraphNet(\n",
       "          (gconv1): LAM_Gconv(\n",
       "            (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (activation): ReLU(inplace=True)\n",
       "          )\n",
       "          (gconv2): LAM_Gconv(\n",
       "            (fc): Linear(in_features=256, out_features=128, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (sublayer): ModuleList(\n",
       "          (0-1): 2 x SublayerConnection(\n",
       "            (norm): LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (gconv_output): ChebConv()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LitGraformer.load_from_checkpoint(model_checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at saved_lifter_2d_3d_model/graphmlp/drive_and_act/A_Pillar_Codriver/predicted_2d/all_actors/lightning_logs/version_0/checkpoints/epoch=34-step=12495.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at saved_lifter_2d_3d_model/graphmlp/drive_and_act/A_Pillar_Codriver/predicted_2d/all_actors/lightning_logs/version_0/checkpoints/epoch=34-step=12495.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8bf40e94da41efb7e49e5aef6cc0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPJPE: 66.88190251588821\n",
      "PJPE\n",
      "                      PJPE\n",
      "nose             55.406219\n",
      "left_eye         55.342503\n",
      "right_eye        53.822598\n",
      "left_ear         13.151522\n",
      "right_ear        38.542057\n",
      "left_shoulder    30.467731\n",
      "right_shoulder   27.312246\n",
      "left_elbow       74.249596\n",
      "right_elbow      66.055267\n",
      "left_wrist       83.904182\n",
      "right_wrist     105.228134\n",
      "left_hip         67.982887\n",
      "right_hip        61.749561\n",
      "activities_mpjpe:\n",
      "{'sitting_still': 63.07036429643631, 'closing_door_inside': 133.79251956939697, 'entering_car': 211.49703860282898, 'opening_bottle': 76.51782780885696, 'closing_bottle': 82.39523321390152, 'drinking': 73.47695529460907, 'pressing_automation_button': 57.17076361179352, 'fetching_an_object': 150.4868119955063, 'eating': 60.81360951066017, 'placing_an_object': 141.84750616550446, 'preparing_food': 132.28167593479156, 'opening_backpack': 192.77071952819824, 'reading_newspaper': 85.44767647981644, 'taking_off_sunglasses': 104.37838733196259, 'using_multimedia_display': 98.5669270157814, 'writing': 94.8110893368721, 'unfastening_seat_belt': 98.53429347276688, 'putting_on_jacket': 222.6172536611557, 'talking_on_phone': 108.59638452529907, 'fastening_seat_belt': 129.11054491996765, 'putting_on_sunglasses': 87.8128930926323, 'reading_magazine': 69.7091743350029, 'taking_off_jacket': 245.84907293319702, 'opening_laptop': 114.86406624317169, 'working_on_laptop': 72.73416966199875, 'looking_or_moving_around (e.g. searching)': 162.84100711345673, 'interacting_with_phone': 77.37758755683899, 'closing_laptop': 122.07571417093277, 'exiting_car': 255.82093000411987, 'taking_laptop_from_backpack': 188.5327398777008, 'putting_laptop_into_backpack': 231.32210969924927, 'opening_door_inside': 164.7968888282776}\n",
      "test mpjpe: 66.88190251588821\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">           mpjpe           </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     66.88190251588821     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m          mpjpe          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    66.88190251588821    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'mpjpe': 66.88190251588821}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'{saved_model_path}/best_model_path.txt', 'w') as f:\n",
    "    f.writelines(model_checkpoint.best_model_path)\n",
    "best_checkpoint_path = model_checkpoint.best_model_path\n",
    "trainer.test(ckpt_path=best_checkpoint_path, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpjpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pressing_automation_button</th>\n",
       "      <td>57.170764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eating</th>\n",
       "      <td>60.813610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sitting_still</th>\n",
       "      <td>63.070364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reading_magazine</th>\n",
       "      <td>69.709174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>working_on_laptop</th>\n",
       "      <td>72.734170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drinking</th>\n",
       "      <td>73.476955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opening_bottle</th>\n",
       "      <td>76.517828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interacting_with_phone</th>\n",
       "      <td>77.377588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closing_bottle</th>\n",
       "      <td>82.395233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reading_newspaper</th>\n",
       "      <td>85.447676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>putting_on_sunglasses</th>\n",
       "      <td>87.812893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing</th>\n",
       "      <td>94.811089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfastening_seat_belt</th>\n",
       "      <td>98.534293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>using_multimedia_display</th>\n",
       "      <td>98.566927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taking_off_sunglasses</th>\n",
       "      <td>104.378387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talking_on_phone</th>\n",
       "      <td>108.596385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opening_laptop</th>\n",
       "      <td>114.864066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closing_laptop</th>\n",
       "      <td>122.075714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastening_seat_belt</th>\n",
       "      <td>129.110545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preparing_food</th>\n",
       "      <td>132.281676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>closing_door_inside</th>\n",
       "      <td>133.792520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>placing_an_object</th>\n",
       "      <td>141.847506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fetching_an_object</th>\n",
       "      <td>150.486812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking_or_moving_around (e.g. searching)</th>\n",
       "      <td>162.841007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opening_door_inside</th>\n",
       "      <td>164.796889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taking_laptop_from_backpack</th>\n",
       "      <td>188.532740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opening_backpack</th>\n",
       "      <td>192.770720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entering_car</th>\n",
       "      <td>211.497039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>putting_on_jacket</th>\n",
       "      <td>222.617254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>putting_laptop_into_backpack</th>\n",
       "      <td>231.322110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taking_off_jacket</th>\n",
       "      <td>245.849073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exiting_car</th>\n",
       "      <td>255.820930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                mpjpe\n",
       "pressing_automation_button                  57.170764\n",
       "eating                                      60.813610\n",
       "sitting_still                               63.070364\n",
       "reading_magazine                            69.709174\n",
       "working_on_laptop                           72.734170\n",
       "drinking                                    73.476955\n",
       "opening_bottle                              76.517828\n",
       "interacting_with_phone                      77.377588\n",
       "closing_bottle                              82.395233\n",
       "reading_newspaper                           85.447676\n",
       "putting_on_sunglasses                       87.812893\n",
       "writing                                     94.811089\n",
       "unfastening_seat_belt                       98.534293\n",
       "using_multimedia_display                    98.566927\n",
       "taking_off_sunglasses                      104.378387\n",
       "talking_on_phone                           108.596385\n",
       "opening_laptop                             114.864066\n",
       "closing_laptop                             122.075714\n",
       "fastening_seat_belt                        129.110545\n",
       "preparing_food                             132.281676\n",
       "closing_door_inside                        133.792520\n",
       "placing_an_object                          141.847506\n",
       "fetching_an_object                         150.486812\n",
       "looking_or_moving_around (e.g. searching)  162.841007\n",
       "opening_door_inside                        164.796889\n",
       "taking_laptop_from_backpack                188.532740\n",
       "opening_backpack                           192.770720\n",
       "entering_car                               211.497039\n",
       "putting_on_jacket                          222.617254\n",
       "putting_laptop_into_backpack               231.322110\n",
       "taking_off_jacket                          245.849073\n",
       "exiting_car                                255.820930"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lit_model.test_history[0]['activities_mpjpe'], index=['mpjpe']).T.sort_values('mpjpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpjpe    128.497498\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(lit_model.test_history[0]['activities_mpjpe'], index=['mpjpe']).T.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[347.3514, 286.7826, 657.2451, 620.2427]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LitGraformer.forward() missing 1 required positional argument: 'batch_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_root_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_figsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_figsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_idices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_plot_gt_skeleton\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/modules/utils/visualization.py:236\u001b[0m, in \u001b[0;36mplot_samples\u001b[0;34m(dataset_root_path, model, dataloader, data_subset, img_figsize, plot_figsize, sample_idices, is_plot_gt_skeleton)\u001b[0m\n\u001b[1;32m    234\u001b[0m scale_factor \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_factor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    235\u001b[0m valid \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 236\u001b[0m estimated_pose \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeypoints2D\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m keypoints_3d \u001b[38;5;241m=\u001b[39m estimated_pose[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    239\u001b[0m img_path \u001b[38;5;241m=\u001b[39m (dataset_root_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m/\u001b[39m data_subset \u001b[38;5;241m/\u001b[39m sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilenames\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mas_posix()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: LitGraformer.forward() missing 1 required positional argument: 'batch_idx'"
     ]
    }
   ],
   "source": [
    "plot_samples(\n",
    "    dataset_root_path,\n",
    "    trainer.model,\n",
    "    test_loader,\n",
    "    'train',\n",
    "    img_figsize=(20, 10),\n",
    "    plot_figsize=(20.5, 10),\n",
    "    sample_idices=[1000, 2500, 6000],\n",
    "    is_plot_gt_skeleton=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
