{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modules.lifter_2d_3d.dataset.gan_keypoint_dataset import GANKeypointDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from modules.lifter_2d_3d.utils.normalization import (\n",
    "    center_pose2d_to_neck,\n",
    "    center_pose3d_to_neck,\n",
    "    normalize_2d_pose_to_image,\n",
    "    normalize_2d_pose_to_bbox,\n",
    "    normalize_2d_pose_to_pose,\n",
    "    normalize_rotation,\n",
    "    rotate2D_to_x_axis\n",
    ")\n",
    "\n",
    "\n",
    "class GANKeypointDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pose_2d,\n",
    "        pose_3d,\n",
    "        actors=None,\n",
    "        exclude_ankle=True,\n",
    "        exclude_knee=True,\n",
    "        is_silence=True,\n",
    "        is_center_to_neck=False,\n",
    "        is_normalize_to_bbox=False,\n",
    "        is_normalize_to_pose=False,\n",
    "        is_normalize_rotation=None,\n",
    "        bbox_format='xywh',\n",
    "        remove_activities=None,\n",
    "        is_gt_2d_pose=False,\n",
    "        subset_percentage=100\n",
    "    ):\n",
    "        self.input_pose_2d = pose_2d\n",
    "        self.input_pose_3d = pose_3d\n",
    "        self.exclude_ankle = exclude_ankle\n",
    "        self.exclude_knee = exclude_knee\n",
    "        self.exclude_ankle = exclude_ankle\n",
    "        self.exclude_knee = exclude_knee\n",
    "        self.is_center_to_neck = is_center_to_neck\n",
    "        self.is_silence = is_silence\n",
    "        self.is_normalize_to_bbox = is_normalize_to_bbox\n",
    "        self.is_normalize_to_pose = is_normalize_to_pose\n",
    "        self.is_gt_2d_pose = is_gt_2d_pose\n",
    "        if (is_normalize_to_pose and is_normalize_to_bbox):\n",
    "            raise ValueError(\n",
    "                'is_normalize_to_pose and ' +\n",
    "                'is_normalize_to_bbox cannot be both true.'\n",
    "            )\n",
    "        self.bbox_format = bbox_format\n",
    "        self.is_normalize_rotation = is_normalize_rotation\n",
    "        self.actors = actors\n",
    "        self.remove_activities = remove_activities\n",
    "        subset_percentage /= 100\n",
    "        self.subset_percentage = subset_percentage\n",
    "        if remove_activities is None:\n",
    "            self.remove_activities = []\n",
    "        self.pose_2d = []\n",
    "        self.pose_3d = []\n",
    "        self.pose_3d_valid = []\n",
    "        self.preprocess_2d()\n",
    "        self.preprocess_3d()\n",
    "\n",
    "    def preprocess_2d(self):\n",
    "        for pose_2d in self.input_pose_2d:\n",
    "            if self.exclude_ankle:\n",
    "                pose_2d = pose_2d[:-2]\n",
    "            if self.exclude_knee:\n",
    "                pose_2d = pose_2d[:-2]\n",
    "            # Drive & Act dataset specify unannotated joints\n",
    "            # with a zero vector\n",
    "            root_2d = np.array([0, 0])\n",
    "            if self.is_center_to_neck:\n",
    "                pose_2d, root_2d = center_pose2d_to_neck(pose_2d)\n",
    "            if self.is_normalize_to_pose:\n",
    "                pose_2d, w, h = normalize_2d_pose_to_pose(pose_2d)\n",
    "            if self.is_normalize_rotation:\n",
    "                # pose_2d, pose_3d = normalize_rotation(pose_2d, pose_3d)\n",
    "                out_pose2d = np.copy(pose_2d)\n",
    "                left_shoulder_index = 5\n",
    "                right_shoulder_index = 6\n",
    "                out_pose2d[:, :2], rotation_matrix = rotate2D_to_x_axis(\n",
    "                    out_pose2d[left_shoulder_index: right_shoulder_index + 1, :2],\n",
    "                    out_pose2d[:, :2]\n",
    "                )\n",
    "            self.pose_2d.append(out_pose2d)\n",
    "\n",
    "    def preprocess_3d(self):\n",
    "        for pose_3d in self.input_pose_3d:\n",
    "            if self.exclude_ankle:\n",
    "                pose_3d = pose_3d[:-2]\n",
    "            if self.exclude_knee:\n",
    "                pose_3d = pose_3d[:-2]\n",
    "            # Drive & Act dataset specify unannotated joints\n",
    "            # with a zero vector\n",
    "            valid_kp = (pose_3d.sum(axis=1) != 0)\n",
    "            if not np.any(valid_kp):\n",
    "                continue\n",
    "            # left_shoulder_index=5, right_shoulder_index=6\n",
    "            # root_3d = np.array([0, 0, 0])\n",
    "            # scale by the image resolution\n",
    "            if self.is_center_to_neck:\n",
    "                pose_3d, root_3d = center_pose3d_to_neck(pose_3d)\n",
    "            self.pose_3d.append(dict(\n",
    "                pose_3d=pose_3d,\n",
    "                valid=valid_kp\n",
    "            ))\n",
    "        np.random.shuffle(self.pose_3d)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pose_3d)\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        pose_2d = self.pose_2d[idx]\n",
    "        pose_3d_ann = self.pose_3d[idx]\n",
    "        pose_3d = pose_3d_ann['pose_3d']\n",
    "        valid = pose_3d_ann['valid']\n",
    "        item = dict(\n",
    "            keypoints_2d=pose_2d[:, :2].astype(np.float32),\n",
    "            keypoints_3d=pose_3d.astype(np.float32),\n",
    "            valid=valid\n",
    "        )\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "keypoint_2d_path = ''\n",
    "keypoint_3d_path = ''\n",
    "\n",
    "synthetic_cabin_ir_1m_root_path = Path('/root/data/processed/synthetic_cabin_1m/') / 'all_views'\n",
    "drive_and_act_root_path = Path('/root/data/processed/drive_and_act/') / 'inner_mirror'\n",
    "\n",
    "synthetic_cabin_ir_1m_keypoint_2d_path = synthetic_cabin_ir_1m_root_path / 'annotations' / 'person_keypoints_train.json'\n",
    "drive_and_act_keypoint_2d_path = drive_and_act_root_path / 'keypoint_detection_results' / 'keypoint_detection_train.json'\n",
    "keypoint_3d_path = synthetic_cabin_ir_1m_root_path / 'annotations'\n",
    "bbox_path = synthetic_cabin_ir_1m_root_path / 'person_detection_results'\n",
    "\n",
    "prediction_files = [\n",
    "    synthetic_cabin_ir_1m_keypoint_2d_path.as_posix(),\n",
    "    drive_and_act_keypoint_2d_path.as_posix(),\n",
    "]\n",
    "annotation_files = [\n",
    "    keypoint_3d_path.as_posix()\n",
    "]\n",
    "\n",
    "\n",
    "# dataset = GANKeypointDataset(\n",
    "#     keypoint_2d_files=prediction_files,\n",
    "#     keypoint_3d_files=annotation_files,\n",
    "# )\n",
    "\n",
    "# dataset.read_prediction_file()\n",
    "\n",
    "# predictions = dataset.read_prediction_file()\n",
    "# bbox_info = dataset.read_bbox_file()\n",
    "# annotation_info = dataset.read_annotation_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(keypoint_3d_path / 'annotations' / 'person_keypoints_train.json') as f:\n",
    "#     data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import pickle\n",
    "\n",
    "# with open(synthetic_cabin_ir_1m_root_path / 'annotations' / 'person_keypoints_train.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_2d = data['annotations'][0]['keypoints']\n",
    "pose_3d = data['annotations'][0]['keypoints3D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_prediction_file(self):\n",
    "#     predictions = {}\n",
    "#     with open(self.prediction_file) as f:\n",
    "#         data = json.loads(f.read())\n",
    "#         if self.is_gt_2d_pose:\n",
    "#             data = data['annotations']\n",
    "#         for item in data:\n",
    "#             predictions[item['image_id']] = item\n",
    "#     return predictions\n",
    "\n",
    "# def read_bbox_file(self):\n",
    "#     bbox_info = {}\n",
    "#     with open(self.bbox_file) as f:\n",
    "#         data = json.loads(f.read())\n",
    "#         for item in data:\n",
    "#             bbox_info[item['image_id']] = item\n",
    "#     return bbox_info\n",
    "\n",
    "# def read_annotation_file(self):\n",
    "#     with open(self.annotation_file) as f:\n",
    "#         data = json.loads(f.read())\n",
    "#         metadata = data['categories'][0]\n",
    "#         camera_parameters = data['camera_parameters']\n",
    "#         images = data['images']\n",
    "#         if self.actors is not None:\n",
    "#             images = [img for img in data['images'] if img['actor'] in self.actors]\n",
    "#         image_annotation_info = {item['id']: item for item in data['annotations']}\n",
    "#     return {\n",
    "#         'metadata': metadata,\n",
    "#         'camera_parameters': camera_parameters,\n",
    "#         'images': images,\n",
    "#         'image_annotation_info': image_annotation_info\n",
    "#     }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
